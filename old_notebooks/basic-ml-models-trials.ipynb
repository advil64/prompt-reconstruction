{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 5.43%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'word-differences.csv'  # Update this to the path of your uploaded file\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Selecting 'Differences' as input and 'Labels_OG' as output\n",
    "X = df['Differences'].astype(str)  # Convert to string to ensure text processing\n",
    "y = df['Labels_OG']\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initializing the TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000)  # Limiting to 1000 features for simplicity\n",
    "\n",
    "# Fitting TF-IDF to the data and transforming our text column into TF-IDF vectors\n",
    "X_tfidf = tfidf.fit_transform(X).toarray()\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CatBoostClassifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.0001,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=False,  # Set to True to see CatBoost's training output\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T23:32:49.274566Z",
     "start_time": "2024-04-10T23:24:25.765728Z"
    }
   },
   "id": "1ca0e73f8552d6c",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 43\u001B[0m\n\u001B[1;32m     40\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(estimator\u001B[38;5;241m=\u001B[39mmodel, param_grid\u001B[38;5;241m=\u001B[39mparam_grid, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Perform the grid search to find the best parameters\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m \u001B[43mgrid_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Best parameters found\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgrid_search\u001B[38;5;241m.\u001B[39mbest_params_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1150\u001B[0m     )\n\u001B[1;32m   1151\u001B[0m ):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    892\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m    893\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m    894\u001B[0m     )\n\u001B[1;32m    896\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m--> 898\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m    902\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1422\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1420\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1421\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1422\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    837\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    838\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    839\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    840\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    841\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    842\u001B[0m         )\n\u001B[1;32m    843\u001B[0m     )\n\u001B[0;32m--> 845\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    858\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    864\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    865\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    866\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    867\u001B[0m     )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     64\u001B[0m )\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    125\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:729\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[1;32m    727\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[1;32m    728\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 729\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    731\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    732\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[1;32m    733\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/catboost/core.py:5100\u001B[0m, in \u001B[0;36mCatBoostClassifier.fit\u001B[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001B[0m\n\u001B[1;32m   5097\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss_function\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m params:\n\u001B[1;32m   5098\u001B[0m     CatBoostClassifier\u001B[38;5;241m.\u001B[39m_check_is_compatible_loss(params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss_function\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m-> 5100\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcat_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbaseline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_best_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5101\u001B[0m \u001B[43m          \u001B[49m\u001B[43meval_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogging_level\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_description\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_period\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5102\u001B[0m \u001B[43m          \u001B[49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_snapshot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshot_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msnapshot_interval\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_cout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_cerr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5103\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/catboost/core.py:2319\u001B[0m, in \u001B[0;36mCatBoost._fit\u001B[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001B[0m\n\u001B[1;32m   2315\u001B[0m allow_clear_pool \u001B[38;5;241m=\u001B[39m train_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_clear_pool\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   2317\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m log_fixup(log_cout, log_cerr), \\\n\u001B[1;32m   2318\u001B[0m     plot_wrapper(plot, plot_file, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTraining plots\u001B[39m\u001B[38;5;124m'\u001B[39m, [_get_train_dir(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_params())]):\n\u001B[0;32m-> 2319\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2320\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_pool\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2321\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meval_sets\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2322\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2323\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_clear_pool\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2324\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_params\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minit_model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m   2325\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2327\u001B[0m \u001B[38;5;66;03m# Have property feature_importance possibly set\u001B[39;00m\n\u001B[1;32m   2328\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_object\u001B[38;5;241m.\u001B[39m_get_loss_function_name()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/catboost/core.py:1723\u001B[0m, in \u001B[0;36m_CatBoostBase._train\u001B[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001B[0m\n\u001B[1;32m   1722\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_train\u001B[39m(\u001B[38;5;28mself\u001B[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001B[0;32m-> 1723\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_object\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_pool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_pool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_clear_pool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_object\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minit_model\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   1724\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_trained_model_attributes()\n",
      "File \u001B[0;32m_catboost.pyx:4645\u001B[0m, in \u001B[0;36m_catboost._CatBoost._train\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_catboost.pyx:4694\u001B[0m, in \u001B[0;36m_catboost._CatBoost._train\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'word-differences.csv'  # Update this to the path of your uploaded file\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Selecting 'Differences' as input and 'Labels_OG' as output\n",
    "X = df['Differences'].astype(str)  # Convert to string to ensure text processing\n",
    "y = df['Labels_OG']\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Initializing the TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000)  # Limiting to 1000 features for simplicity\n",
    "\n",
    "# Fitting TF-IDF to the data and transforming our text column into TF-IDF vectors\n",
    "X_tfidf = tfidf.fit_transform(X).toarray()\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Setting up a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'iterations': [100, 200],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'depth': [4, 6, 8],\n",
    "}\n",
    "\n",
    "# Initialize the CatBoostClassifier\n",
    "model = CatBoostClassifier(loss_function='MultiClass', verbose=False)\n",
    "\n",
    "# Setting up the grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy with Best Model: {accuracy * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T00:54:25.309684Z",
     "start_time": "2024-04-10T23:45:27.481912Z"
    }
   },
   "id": "3f9df737de467f7c",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.90%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'word-differences.csv'  # Update this to the path of your uploaded file\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Selecting 'Differences' as input and 'Labels_OG' as output\n",
    "X = df['Differences'].astype(str).apply(lambda x: x.split())  # Tokenize the text\n",
    "y = df['Labels_OG']\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=X, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to vectorize a document by averaging its word vectors\n",
    "def document_vector(doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.index_to_key]\n",
    "    if not doc:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)\n",
    "\n",
    "# Vectorize the documents\n",
    "X_vectorized = np.array([document_vector(doc) for doc in X])\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CatBoostClassifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.0001,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=False,  # Set to True to see CatBoost's training output\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T00:58:48.400483Z",
     "start_time": "2024-04-11T00:54:28.849226Z"
    }
   },
   "id": "b7b57827b44066a3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 2.66%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'word-differences.csv'  # Update this to the path of your uploaded file\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Selecting 'Differences' as input and 'Labels_OG' as output\n",
    "X = df['Differences'].astype(str)\n",
    "y = df['Labels_OG']\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Prepare the data for Doc2Vec\n",
    "tagged_data = [TaggedDocument(words=_d.split(), tags=[str(i)]) for i, _d in enumerate(X)]\n",
    "\n",
    "# Train a Doc2Vec model\n",
    "vector_size = 100  # Dimensionality of the document vectors\n",
    "window = 5\n",
    "min_count = 1\n",
    "\n",
    "model = Doc2Vec(tagged_data, vector_size=vector_size, window=window, min_count=min_count, workers=4, epochs=40)\n",
    "\n",
    "# Create document vectors\n",
    "X_vectorized = np.array([model.dv[str(i)] for i in range(len(X))])\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CatBoostClassifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.0001,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=False,  # Set to True to see CatBoost's training output\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T01:03:22.401263Z",
     "start_time": "2024-04-11T00:59:24.243885Z"
    }
   },
   "id": "e2bb393c5da82bb",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a53f05e8834426597ed21eb28845ccb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'word-differences.csv'  # Update this to the path of your uploaded file\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Selecting 'Differences' as input and 'Labels_OG' as output\n",
    "X = df['Differences'].astype(str)\n",
    "y = df['Labels_OG']\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Load pre-trained model tokenizer and model (for embeddings)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization and Encoding the dataset for BERT\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Extract embeddings function\n",
    "def extract_embeddings(encoded_input):\n",
    "    with torch.no_grad():\n",
    "        features = model(**encoded_input).last_hidden_state[:, 0, :].numpy()\n",
    "    return features\n",
    "\n",
    "# Encoding texts\n",
    "encoded_input = encode_texts(X.tolist())\n",
    "\n",
    "# Extracting embeddings\n",
    "X_embeddings = extract_embeddings(encoded_input)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Using Logistic Regression as the classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-11T01:03:42.918135Z"
    }
   },
   "id": "5feb888331f054c1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'word-differences.csv'  # Update this to the path of your uploaded file\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Selecting 'Differences' as input and 'Labels_OG' as output\n",
    "X = df['Differences'].astype(str)\n",
    "y = df['Labels_OG']\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Load pre-trained model tokenizer and model (for embeddings)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization and Encoding the dataset for BERT\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Extract embeddings function\n",
    "def extract_embeddings(encoded_input):\n",
    "    with torch.no_grad():\n",
    "        # Output is tuple, get the hidden states from the first element\n",
    "        features = model(**encoded_input).last_hidden_state\n",
    "        # Use mean pooling to get a single vector embedding from the output token embeddings\n",
    "        features = features.mean(dim=1)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Encoding texts\n",
    "encoded_texts = encode_texts(X.tolist())\n",
    "\n",
    "# Extracting embeddings\n",
    "X_embeddings = extract_embeddings(encoded_texts)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CatBoostClassifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.0001,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=False,  # Set to True to see CatBoost's training output\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-11T02:04:41.672732Z"
    }
   },
   "id": "c5676739b12e1b30"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'word-differences.csv'  # Update this to the path of your uploaded file\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Selecting 'Differences' as input and 'Labels_OG' as output\n",
    "X = df['Differences'].astype(str)\n",
    "y = df['Labels_OG']\n",
    "\n",
    "# Encoding the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Load pre-trained model tokenizer and model (for embeddings)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure padding token is set for the GPT2 tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization and Encoding the dataset for GPT\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# Extract embeddings function\n",
    "def extract_embeddings(encoded_input):\n",
    "    with torch.no_grad():\n",
    "        # Output is tuple, get the hidden states from the first element\n",
    "        features = model(**encoded_input).last_hidden_state\n",
    "        # Use mean pooling to get a single vector embedding from the output token embeddings\n",
    "        features = features.mean(dim=1)\n",
    "    return features.cpu().numpy()\n",
    "\n",
    "# Encoding texts\n",
    "encoded_texts = encode_texts(X.tolist())\n",
    "\n",
    "# Extracting embeddings\n",
    "X_embeddings = extract_embeddings(encoded_texts)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the CatBoostClassifier\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.0001,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    verbose=False,  # Set to True to see CatBoost's training output\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "catboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = catboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-11T15:55:36.407292Z"
    }
   },
   "id": "e7487d0a0117b0f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6f9d60dab8637024"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
